{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c53e18-cf19-447b-bcba-060ff1f60575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "import bokeh\n",
    "import bokeh.io\n",
    "import bokeh.plotting\n",
    "from datetime import datetime as dt\n",
    "import dateutil.parser\n",
    "import shutil\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Javascript\n",
    "import os\n",
    "from pyspark.sql.session import SparkSession\n",
    "from bokeh.embed import components\n",
    "bokeh.io.output_notebook(hide_banner=False)\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.palettes import brewer, d3\n",
    "from pyspark.sql.types import StructType\n",
    "import json\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"2G\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92a2f3d-6e28-41a7-89ef-077b9d01050d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed07df64-aa9a-4e8d-b8dc-193897cbb9e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "measures = [\n",
    "        'worker_id',\n",
    "        'triggerExecution',\n",
    "        'commitTime',\n",
    "        'memoryUsedMiB',\n",
    "        'diskUsedGiB',\n",
    "        #'rocksdbSstFileSizeGiB',\n",
    "        #'allUpdatesTime',\n",
    "        #'allRemovalsTime',\n",
    "        'numRowsTotal'\n",
    "        ]\n",
    "\n",
    "def get_path(experiment):\n",
    "    path = f\"{os.getcwd()}/telemetry/{experiment}.log.json\"\n",
    "    if os.path.exists(path):\n",
    "        return f\"file://{path}\"\n",
    "    else:\n",
    "        return f\"file://{path}.gz\"\n",
    "\n",
    "def read_metric(experiment):\n",
    "    schema = None\n",
    "    with open(\"metric_schema.json\", \"r\") as f:\n",
    "        data = f.read()\n",
    "        json.loads(data)\n",
    "        schema = StructType.fromJson(json.loads(data))\n",
    "\n",
    "    spark.read.json(get_path(experiment), schema=schema).createOrReplaceTempView(\"metrics_table\")\n",
    "    \n",
    "    df = spark.sql(f\"\"\"\n",
    "    --start-sparksql\n",
    "    select ts,\n",
    "    experiment,\n",
    "        triggerExecution,\n",
    "        memoryUsedMiB,\n",
    "        commitTime,\n",
    "        worker_id,\n",
    "        numRowsTotal,\n",
    "        jvmMemoryUsedMiB,\n",
    "        numPut,\n",
    "        numGet,\n",
    "        serializationTime,\n",
    "        deserializationTime,\n",
    "        sortTime,\n",
    "        putAndGetTagTime,\n",
    "        numPut / putAndGetTagTime as numPutPerSec,\n",
    "        numGet / putAndGetTagTime as numGetPerSec\n",
    "        from (\n",
    "        select\n",
    "            *,\n",
    "            timestamp(timestamp) as ts,\n",
    "            '{experiment}' as experiment,\n",
    "            durationMs.triggerExecution / 1000 as triggerExecution,\n",
    "            s.memoryUsedBytes,\n",
    "            s.memoryUsedBytes / 1024 / 1024 as memoryUsedMiB,\n",
    "            s.commitTimeMs,\n",
    "            s.commitTimeMs / 1000 as commitTime,\n",
    "            disk_used / 1024 / 1024 / 1024 as diskUsedGiB, \n",
    "            coalesce(s.customMetrics.putTagCount, 0) / 16 as numPut,\n",
    "            coalesce(s.customMetrics.getTagCount, 0) / 16 as numGet,\n",
    "            coalesce(s.customMetrics.sortTimer, 0) / 1000 / 16 as sortTime,\n",
    "            coalesce(s.customMetrics.toStateTimer, 0) / 1000 / 16 as serializationTime,\n",
    "            coalesce(s.customMetrics.fromStateTimer, 0) / 1000 / 16 as deserializationTime,\n",
    "            coalesce(s.customMetrics.updateTagCacheTimer, 0) / 1000 / 16 as putAndGetTagTime,\n",
    "            s.customMetrics.rocksdbSstFileSize,\n",
    "            s.customMetrics.rocksdbSstFileSize / 1024 / 1024 / 1024 as rocksdbSstFileSizeGiB,\n",
    "            s.allUpdatesTimeMs,\n",
    "            s.allUpdatesTimeMs / 1000 as allUpdatesTime,\n",
    "            s.allRemovalsTimeMs,\n",
    "            s.allRemovalsTimeMs / 1000 as allRemovalsTime,\n",
    "            s.numRowsTotal,\n",
    "            split(worker_name, '-')[4] as worker_id,\n",
    "            executor_memory / 1024 / 1024 as jvmMemoryUsedMiB\n",
    "        from\n",
    "            (\n",
    "                select\n",
    "                    *,\n",
    "                    stateOperators[0] as s\n",
    "                from\n",
    "                    metrics_table\n",
    "                where\n",
    "                    timestamp is not null\n",
    "            )\n",
    "        )\n",
    "    --end-sparksql\n",
    "    \"\"\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def apply_sql(df, view, sql):\n",
    "    df.createOrReplaceTempView(view)\n",
    "    return spark.sql(sql)\n",
    "\n",
    "def render_experiments(experiments, measures):\n",
    "    dataframes = []\n",
    "    for experiment in experiments:\n",
    "        df = read_metric(experiment)\n",
    "        #df.persist()\n",
    "        dataframes.append(df)\n",
    "    for measure in measures:\n",
    "        if measure == 'worker_id':\n",
    "            render_experiment(experiments, dataframes, measure, 'circle')\n",
    "        else:\n",
    "            render_experiment(experiments, dataframes, measure, 'line')\n",
    "    for df in dataframes:\n",
    "        df.unpersist()\n",
    "            \n",
    "def render_experiment(experiments, dataframes, measure, plot_type):\n",
    "    p = figure(x_axis_label='ts', y_axis_label=measure, x_axis_type='datetime', plot_width=1200, plot_height=400)\n",
    "    for idx, experiment in enumerate(experiments):\n",
    "        df = dataframes[idx]\n",
    "        x = [r['ts'] for r in df.collect()]\n",
    "        y = [r[measure] for r in df.collect()]\n",
    "        color = d3['Category20'][20][idx]\n",
    "        if plot_type == 'line':\n",
    "            p.line(x, y, line_color=color, legend_label=experiment, line_width=2)\n",
    "        else:\n",
    "            p.circle(x, y, color=color, legend_label=experiment, size=2)\n",
    "    p.legend.background_fill_alpha = 0.5\n",
    "    p.legend.click_policy=\"hide\"\n",
    "    show(p)\n",
    "\n",
    "def print_workers(experiments):\n",
    "    for experiment in experiments:\n",
    "        view = experiment\n",
    "        sql = f\"\"\"--start-sparksql\n",
    "            select\n",
    "                t.experiment,\n",
    "                t.worker_name,\n",
    "                split(t.worker_name, '-')[4] as worker_id,\n",
    "                min(t.timestamp) as min_timestamp,\n",
    "                max(t.timestamp) as max_timestamp\n",
    "            from\n",
    "                {view} as t\n",
    "            group by\n",
    "                t.experiment, t.worker_name\n",
    "            order by\n",
    "                max_timestamp asc\n",
    "        --end-sparksql\"\"\"\n",
    "        df = read_metric(experiment)\n",
    "        df = apply_sql(df, view, sql)\n",
    "        df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828871d-0ce9-45d6-9f0d-5ed0c334c418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "measures = [\n",
    "        'triggerExecution',\n",
    "        'jvmMemoryUsedMiB',\n",
    "        'commitTime',\n",
    "        'memoryUsedMiB',\n",
    "        #'worker_id',\n",
    "        #'numRowsTotal',\n",
    "        'numPutPerSec',\n",
    "        'numGetPerSec',\n",
    "        'putAndGetTagTime',\n",
    "        'numPut',\n",
    "        'numGet',\n",
    "        'serializationTime', \n",
    "        'deserializationTime', \n",
    "        'sortTime'\n",
    "]\n",
    "#bloom_r5000_p16_a50000_b50_f200000002_t60_x45G_hdfs -> 1 version\n",
    "#bloom_r5000_p16_a50000_b50_f200000003_t60_x45G_hdfs -> 0 version\n",
    "\n",
    "experiments = [\n",
    "#     \"bloom_r5000_p16_a50000_b500_f20000000_t60_x45G_hdfs_javaser\",\n",
    "#     \"bloom_r5000_p16_a50000_b50000_f200000_t60_x45G_hdfs_javaser\",\n",
    "#     \"bloom_r5000_p16_a50000_b500_f20000000_t60_x45G_hdfs\",\n",
    "#     \"bloom_r5000_p16_a50000_b50000_f200000_t60_x45G_hdfs\",\n",
    "#     \"bloom_r5000_p16_a50000_b50000_f300000_t60_x45G_hdfs\",\n",
    "    \n",
    "    #\"bloom_r10000_p16_a50000_b500_f30000000_t60_x30G_fluxstore\",\n",
    "    #\"bloom_r10000_p16_a50000_b500_f50000000_t60_x45G_fluxstore\",\n",
    "    #\"bloom_r10000_p16_a50000_b500_f40000000_t60_x45G_fluxstore\",\n",
    "    #\"bloom_r10000_p16_a50000_b500_f5000000_t60_x45G_fluxstore\",\n",
    "    #\"bloom_r10000_p16_a50000_b50000_f500000_t60_x45G_fluxstore\",\n",
    "    #\"bloom_r5000_p16_a50000_b50000_f500000_t60_x45G_fluxstore\",\n",
    "    #\"bloom_r10000_p16_a50000_b500_f60000000_t60_x45G_fluxstore\",\n",
    "    #\"bloom_r10000_p16_a50000_b50000_f200000_t60_x45G_fluxstorecomp\",\n",
    "    #\"bloom_r10000_p16_a50000_b50000_f200000_t60_x45G_fluxstore\",\n",
    "    #\"bloom_r10000_p16_a50000_b50000_f200000_t60_x45G_hdfs\",\n",
    "    \n",
    "    #\"bloom_r5000_p16_a50000_b50000_f200000_t60_x45G_hdfs\",\n",
    "    #\"bloom_r20000_p16_a50000_b50000_f200000_t60_x45G_fluxstore\",\n",
    "    #\"bloom_r5000_p16_a50000_b50000_f600000_t60_x45G_fluxstore\",\n",
    "    \n",
    "#     \"bloom_r5000_p16_a50000_b50000_f200000_t60_x45G_hdfs\",\n",
    "#     \"bloom_r5000_p16_a50000_b50000_f300000_t60_x45G_fluxstore\",\n",
    "#     \"bloom_r8000_p16_a50000_b50000_f200000_t60_x45G_fluxstore\",\n",
    "    \n",
    "#     \"bloom_r5000_p16_a50000_b500_f20000000_t60_x45G_hdfs\",\n",
    "#     \"bloom_r5000_p16_a50000_b500_f30000000_t60_x45G_fluxstore\",\n",
    "#     \"bloom_r8000_p16_a50000_b500_f20000000_t60_x45G_fluxstore\",\n",
    "    \n",
    "#     \"bloom_r5000_p16_a50000_b500_f30000000_t60_x45G_fluxstorecomp\",\n",
    "#     \"bloom_r8000_p16_a50000_b500_f20000000_t60_x45G_fluxstorecomp\",\n",
    "    \n",
    "    #\"bloom_r10000_p16_a50000_b50000_f400000_t60_x45G_fluxstorecomp\",\n",
    "    \"bloom_r10000_p16_a50000_b50000_f400000_t30_x45G_fluxstorecomp\",\n",
    "    \"bloom_r15000_p16_a50000_b50000_f400000_t10_x45G_fluxstorecomp\",\n",
    "]\n",
    "\n",
    "print(experiments)\n",
    "render_experiments(experiments, measures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc05a53e-ef3f-44a7-b802-44a1ca9e5351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bdc9d0-a815-4536-884b-e3fb120ee7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f52cf0-cfca-4265-9f46-8115a446c04a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4750f1-e07e-4569-93e2-7423faf44bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1d7140-1753-45a1-a582-7e8b83321688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26016a-f184-47ec-af31-4f8356846d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867abe4-c5dd-4289-a655-9e6404d97d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ed9ab-8dcc-4ca9-ad69-d4344e81d3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
